inception이란?

-   새로운 레이어 구조

-   레이어를 무작정 많이 쌓는 것이 아니라 새로운 구조를 만들어서 사용

lenet, alexnet 으로 발전하며 레이어를 계속 쌓으면서 성능 좋아짐

-   dropout, normalization 등의 시도를 하면서

레이어를 계속 늘리는 것은 자원소모적임

레이어의 형태를 바꾸어 적당히 늘리는 것이 더욱 효율적

-   단순히 쌓고 dropout을 할 바에는 이전에 새로운 구조를 만들면 소모적인
    > 과정을 거칠 필요가 없겠다!

GoogLeNet - 22 layer deep network

-   구글넷의 파라미터 개수를 열두배를 해야 알렉스넷의 파라미터 개수

-   그러나 정확도는 구글넷이 보다 높음

Related Work - Network in Network

-   linear convolution layer vs Mlpconv layer : 선형 vs 비선형

-   convolution하는 이유 : feature를 뽑아낼 때 edge, corner 등을 잘
    > 뽑아내는 데에 있어서 선형 함수로 할 수 있는 일이 아님

one by one convolutional layer

larger size model -\> higher performance

-   drawbacks

    -   크기가 클수록 일반적으로 매개 변수의 수가 많아지므로 특히 훈련
        > 세트의 레이블링된 예제 수가 제한된 경우 확장된 네트워크가
        > 과적합되기 쉽다.

    -   균일하게 증가된 네트워크 크기의 또 다른 단점은 계산 자원의
        > 사용이 기하급수적으로 증가한다는 것이다.

        -   한정된 컴퓨터 성능에서 모델을 돌릴 수 없음

Hebbian principle : 두 뉴런이 연결되는 것이 반복된다면 그 연결이 더욱
강화된다.

-   neurons that fire together, wire together

일반적으로 데이터는 non uniform & sparse data -\> 이런 유형의 데이터를
처리하는 데에 매우 비효율적인 구조임

-   이를 dense하게 만들어야겠다 -\> inception 구조 생성

-   Arora의 연구에서 나온 것 처럼 sparse structure를 dense하게
    > approximate

GoogLeNet의 구조

![](media/image1.png){width="4.523781714785652in"
height="5.205311679790026in"}

-   인셉션 아키텍처의 주요 아이디어는 컨볼루션 비전 네트워크의 최적의
    > 로컬 희소 구조를 쉽게 구할 수 있는 밀도 높은 구성 요소에 의해
    > 근사화되고 커버될 수 있는 방법을 고려하는 것.

-   In order to avoid patch-alignment issues -\> convolution filter의
    > 크기를 1\*1, 3\*3, 5\*5로 한정시킴 (필요성 아닌 편의성 기반)

-   또한 현재 컨볼루션 네트워크의 성공에 있어 풀링 작업이 필수적이므로,
    > 그러한 각 단계에서 대체 병렬 풀링 경로를 추가하는 것도 추가적인
    > 유익한 영향을 미칠 것임

-   이러한 \"인셉션 모듈\"이 서로 겹쳐지면서 출력 상관 통계는 달라질 수
    > 밖에 없다: 더 높은 추상화의 특징들이 더 높은 계층에 의해 포착됨에
    > 따라 공간 밀도가 감소할 것으로 예상된다. 이것은 우리가 더 높은
    > 층으로 이동함에 따라 3×3 및 5×5 컨볼루션의 비율이 증가해야 함을
    > 시사한다.

```{=html}
<!-- -->
```
-   naive 형태에서 모듈의 한 가지 큰 문제는 적은 수의 5×5 컨볼루션도
    > 많은 수의 필터가 있는 컨볼루션 레이어 위에서 엄청나게 비쌀 수
    > 있다는 점이다.

-   이 문제는 풀링 유닛이 혼합에 추가되면 더욱 뚜렷해진다. 출력 필터의
    > 수는 이전 단계의 필터 수와 같다. 풀링 레이어의 출력과 컨볼루션
    > 레이어의 출력을 병합하면 단계 간 출력 수가 불가피하게 증가할 수
    > 있다. 이 아키텍처는 최적의 희소 구조를 커버할 수 있지만, 매우
    > 비효율적으로 수행되어 몇 단계 내에 계산 폭발로 이어질 수 있다.

```{=html}
<!-- -->
```
-   이것은 인셉션 아키텍처의 두 번째 아이디어로 이어진다. 즉, 계산 요구
    > 사항이 지나치게 증가할 경우 신중하게 차원을 줄이는 것이다.

-   이는 임베딩에 기초한다. 저차원 임베딩에도 상대적으로 큰 이미지
    > 패치에 대한 많은 정보가 포함될 수 있다. 그러나 임베딩은 정보를
    > 밀도가 높고 압축된 형태로 나타내며 압축된 정보는 처리하기가 더
    > 어렵다.

-   표현은 대부분의 장소에서 희소하게 유지되어야 하며 신호를 일괄적으로
    > 집계해야 할 때만 압축해야 합니다. 즉, 1×1 컨볼루션은 expensive한
    > 3×3 및 5×5 컨볼루션을 계산하기 이전에 사용된다. 감소로 사용될 뿐만
    > 아니라, 이를 이중 목적으로 만드는 정류된 선형 활성화의 사용도
    > 포함된다.

```{=html}
<!-- -->
```
-   인셉션 네트워크는 위 유형의 모듈들이 서로 겹쳐져 있는 네트워크이며,
    > 그리드의 해상도를 절반으로 줄이기 위해 스트라이드 2가 있는 최대
    > 풀링 레이어를 가끔 포함한다.

-   낮은 레이어의 경우 전통적인 컨볼루션 방식으로 유지하면서 상위
    > 레이어에서만 인셉션 모듈을 사용하는 것이 유익하다.

이전 레이어에서 feature map을 받아서

1\*1, 3\*3, 5\*5 convolution & 3\*3 max pooling 각각 연산하여 이를 모두
concatenate

-\> 이를 naive하게 진행하게 되면, 계산량이 기하급수적으로 늘어남

이를 해결하기 위해 구조의 변형 진행

1\*1 convolution을 먼저 진행하여 3\*3, 5\*5 convolution 계산을 하여
차원의 수를 감소시킴

dimension reduction : 데이터포인트가 줄어들었다.

-   100 \* 100 -\> 100 \* 1 & 1 \* 100 : 데이터포인트의 수가 10000개에서
    > 200개로 줄어듬

-   이러한 예시와 같은 의미로 dimension reduction 이해 가능

-   non inception 모델과 a, b 모델의 비교 필요

\# alexnet params : 6000만개

\# inceptionnet params : 500만개 ; 1/12 수준으로 줄어듦

inception net의 가장 큰 단점 : carefully crafted design

-   모델이 매우 정교함

-   train, test에서의 시간은 단축하였으나 모델을 각 데이터 입력에 맞추어
    > 수정할 때 드는 비용은 고려하지 않음

-   다른 모델과의 fair한 비교가 어려움

동일한 퍼포먼스를 내는 모델에서 computer resource를 줄일 수 있음 but 그
구조가 복잡하여 이를 구현하는 데에 있어서 어려움 있음
