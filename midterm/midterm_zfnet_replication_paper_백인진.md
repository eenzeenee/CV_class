Visualizing and Understanding Convolutional Networks

-   Replication paper

2021312088 백인진

1.  Introduction

> 1990년대 초반 LeCun 등에 의해 소개된 이후, Convolutional Networks는
> 손글씨, 얼굴 감지 등의 작업에서 우수한 성능을 보였다. 더불어 지난
> 18개월 동안 여러 논문에서 더욱 까다로운 시각적 분류 작업에서 또한
> 뛰어난 성과를 보여주었다. Ciresan 등에 의해 NORB, CIFAR 10
> 데이터세트에 대한 최신 성능을 볼 수 있다. 가장 주목할만한 연구는
> Krizhevsky 등의 연구이다. 이들은 ImageNet 2021 classification
> benchmark에서 오류율 16.4%을 보이는 ConvNet을 내보이며 2위의 결과인
> 26.1%에 비해 높은 성능을 보였다. 이후 Girshick 등의 연구진이 PASCAL
> VOC 데이터셋에서 가장 높은 성능을 보였다. 이러한 성능 향상에 여러
> 요인들이 영향을 미쳤다. 해당 요인에는 레이블이 달린 수많은 훈련
> 데이터세트의 가용성, 매우 큰 모델의 교육을 가능하게 하는 강력한 GPU의
> 구현, Dropout과 같은 더 나은 모델 정규화 전략 등이 있다.
>
> 그러나 이러한 발전에도 불구하고, 복잡한 모델의 내부 작동과 그러한 좋은
> 성능을 달성하는 방법에 대한 통찰은 여전히 부족하다. 과학적인 관점에서
> 이는 매우 불만족스러운 일이며, 모델이 어떻게, 왜 작동하는지 명확하게
> 이해하지 못하면 더 나은 모델의 개발은 그저 시행착오에 불과한 일이 될
> 뿐이다. 본 논문에서는 모델의 모든 계층에서 개별적인 feature map을
> 활성시키는 입력 자극을 시각화하는 기술을 보인다. 이를 활용하여 학습
> 과정 중 feature의 변화를 관찰하고 모델의 잠재적인 문제를 진단하게끔
> 한다. 제안된 시각화 기법은 Zeiler 등이 제안한 바와 같이 다층의
> Deconvolutional Network를 사용하여 활성화된 feature를 픽셀 공간에 다시
> 투영한다. 또한 입력 이미지의 일부를 가려 분류기 출력의 민감도 분석을
> 수행하여 분류에 중요한 장면의 일부를 보여준다.
>
> 이러한 도구를 사용하여 Kerizhevsky가 제안한 아키텍처에서 시작하여
> ImageNet에서 결과를 능가하는 아키텍처를 발견해 다양한 아키텍처를
> 탐색하고자 한다. 이후, 상위의 softmax 층을 재훈련하여 모델의 다른
> 데이터셋에 대한 일반화 능력을 탐색한다. 이는 Hinton 등에 의해 대중화된
> unsupervised pre-training 방식과 대조되는 supervised pre-training의 한
> 형태이다.

2.  Approach

> 본 논문에서는 LeCun, Krizhevsky 등의 연구진이 정의한대로 논문 전반에
> 걸쳐 fully supervised ConvNet을 사용한다. 이 모델들은 일련의 레이어를
> 통해 색상 2D 입력 이미지 xi를 클래스 벡터 C에 해당하는 확률 벡터 yi로
> 매핑한다. 각 레이어는 (1) 학습된 필터 세트를 가진 이전 레이어 출력의
> Convolution으로 구성되며 (2) 선형함수, 더불어 선택적으로 (3) 지역적
> 이웃에 대한 maxpooling, (4) feature map에서 응답을 정규화하는 지역
> 대비 작업 등으로 구성된다.
>
> 이러한 모델을 라벨이 붙은 N개의 (x, y) 쌍의 이미지 데이터셋을 사용하여
> 훈련한다. 예측된 값과 실제 정답을 비교하기 위해 교차 엔트로피 손실
> 함수를 사용한다. 네트워크의 매개 변수(컨볼루션 계층의 필터, 완전히
> 연결된 레이어 및 바이어스의 가중치 매트릭스)는 네트워크 전체의 매개
> 변수와 관련하여 손실의 도함수를 역전파하고 확률적 경사 강하를 통해
> 매개 변수를 업데이트하여 학습된다.

A.  Visualization with Deconvnet

> ConvNet의 작동을 이해하기 위해서는 중간 레이어의 feature activity를
> 해석할 수 있어야 한다. 이러한 activity를 입력 픽셀 공간으로 다시
> 매핑하는 새로운 방법을 제시하여 feature map에서 원래 어떤 입력 패턴이
> 해당 활성화를 야기했는지 보여준다. DeconvNet은 기존 Zelier가 제시한
> 모델에 대해 동일한 구성요소 (filtering, pooling)를 사용하는 ConvNet
> 모델이라고 할 수 있으나 입력 픽셀을 feature에 매핑하는 것이 아니라 그
> 반대의 역할을 한다.
>
> ConvNet을 검사하기 위해 DeconvNet을 각 레이어에 부착하여 입력 이미지
> 픽셀로 다시 돌아가는 연속된 경로를 제공한다. 시작하기에 앞서, 입력
> 이미지가 ConvNet에 제공되며 전체 레이어에 걸쳐 feature를 계산한다.
> 주어진 ConvNet을 검사하기 위해, 레이어의 다른 모든 activation을 0으로
> 설정하고 feature map을 연결된 DeconvNet의 입력으로 전달한다. 그런 다음
> (1) unpool, (2) rectify, (3) filter 등을 연속적으로 사용하여 선택한
> activation을 발생시킨 다음 아래 계층의 퐐동을 재구성한다. 이후 다음의
> 입력 픽셀 공간에 도달할때까지 반복한다. 이때, 각각의 기능을 설명하면
> 아래와 같다.

1.  Unpooling : ConvNet에서, max pooling연산은 다시 원래대로 되돌릴 수
    > 없지만, switch 변수 집합에서 각 pooling영역 내의 최대값의 위치를
    > 기록하여 대략적인 복원이 가능하다. DeconvNet에서 unpooling 작업은
    > 이러한 switch 변수들을 자극의 구조를 유지하며 상위 레이어의 적절한
    > 위치에 되돌려놓는다.

2.  Rectification : ConvNet은 비선형의 ReLU 활성화 함수를 사용하는데,
    > 이는 feature map을 수정하며 그것이 항상 양수임을 보장한다. 이때,
    > 각 계층에서 유효하게 재건된 feature를 얻기 위해서는 ReLU
    > 비선형성을 통해 재구성한 신호를 전달한다.

3.  Filter : ConvNet은 학습된 filter를 사용하여 이전 레이어의 feature
    > map을 합성곱한다. 이를 대략적으로 되돌리기 위해 DeconvNet은 RBM과
    > 같은 다른 autoencoder모델과 같이 동일한 필터를 전치하여 사용한다.
    > 그러나 이를 아래의 레이어가 아닌 rectifiy된 feature map에
    > 적용시킨다. 실제로 이는 각 필터를 수직, 수평으로 뒤집은 것을
    > 말한다.

> 재구성 경로에 있는 경우 contrast normalization 작업을 진행하지 않는다.
> 상위 레이어에서 아래로 투영하면 위로 올라갈때 ConvNet의 max pooling에
> 의해 생성된 switch 설정이 사용된다. 이러한 switch 설정은 주어진 입력
> 이미지에 대해 고유하므로 단일 activation에서 얻은 재구성은 feature
> activation에 대한 기여도에 따라 가중치가 부여된 구조와 함께 원래
> 입력의 작은 조각과 유사하게 된다. 생성 과정이 포함되지 않기 대문에
> 이러한 예측은 모형에서 추출한 표본이 아니다. 전체적인 절차는 하나의
> 강력한 activation의 backpropping과 유사하다. 즉, ∂h / ∂Xn를 계산하는
> 것이다. 이때 h는 강력한 activation을 갖는 feature map의 구성요소이고
> Xn은 입력 이미지이다. 그러나, 이는 (1) ReLU가 독립적으로 부과되고(2)
> contrast 정규화 연산이 사용되지 않는다는 점에서 차이를 갖는다. 이러한
> 접근법의 일반적인 단점은 레이어에 존재하는 공통의 활동이 아닌 단일
> activation에 대해서만 시각화한다는 것이다. 그럼에도 불구하고 이러한
> 시각화는 모델에서 주어진 feature map을 자극하는 입력 패턴을 정확하게
> 표현 가능하다. 즉, 패턴에 해당하는 원래 입력 영상의 일부가 가려지면
> feature map에서도 뚜렷한 activity 감소를 확인할 수 있다.

3.  Training Details

> 실험에 사용할 구조는 Krizhevsky 연구진이 ImageNet 이미지 분류를 위해
> 제안한 구조와 매우 유사하다. 한가지 차이점은 2개의 GPU로 분할된 모델
> 구조로 인해 나타난 레이어 3, 4, 5에 사용된 희소 연결이 본 논문의
> 모델에서 고밀도의 연결로 대체된다는 점이다.
>
> 이 모델은 ImageNet 2012 학습 데이터셋(이미지 130만개, 클래스 1000개)에
> 대해 훈련되었다. 각 RGB 이미지는 가로 세로 중 가장 작은 치수를 256으로
> 조정한 뒤 중심의 256\*256 영역을 잘라내어 각 픽셀에 대해 이미지
> 평균값을 빼고 224\*224 크기의 하위 크롭 이미지를 사용하여 전처리된다.
> 확률적 경사 하강법에서 mini batch 크기를 128로 설정하고 momentum
> term을 0.9로, learning rate를 0.01로 설정하고 학습을 시작하였다.
> 더불어 검증 오류가 발생할 때에는 수동으로 learning rate를 조절하며
> 학습하였다. Dropout은 fully connected layer인 6, 7번 레이어에서 0.5로
> 사용되고 모든 가중치는 0.01, 편향은 0으로 초기화된다.
>
> 훈련 중에 첫번째 레이어 필터를 시각화하면 그중의 일부가 dominate임을
> 확인할 수 있다. 이를 방지하기 위해 RMS값이 고정 반지름 0.1을 초과하는
> Convolution 레이어의 필터들을 다시 0.1 고정 반지름으로 정규화한다.
> 이는 입력이미지가 대략적으로 \[-128, 128\] 범위에 있는 모델의 첫번째
> 레이어에서 특히 중요한 과정이다. Krizhevsky의 연구와 같이, 훈련
> 데이터셋의 크기를 늘리기 위해 각 훈련 예제의 여러가지 크롭과 플립을
> 제작한다. 단일 GTX580 GPU에서 70 에폭을 학습한 뒤 중단하였다.

4.  Convent Visualization

> 섹션 3에서 설명한 모델을 사용하여 DeconvNet을 활용해 ImageNet 검증
> 데이터셋에서 feature activation을 확인하고자 한다.

1.  Feature Visualization : 학습 완료 후 각 feature map을 보여주는
    > 그림을 그려보면, 주어진 feature map의 경우 각 activation은 픽셀
    > 공간에 별도로 투영되어 해당 맵을 활성화하는데 다양한 구조를
    > 드러내고 입력의 변형에 대한 불변성을 보여준다. 이러한 시각화와
    > 함께 해당하는 이미지 패치를 보여준다. 이들은 각 패치 내 판별
    > 구조에만 초점을 맞춘 시각화보다 더 큰 변화를 가진다. 또한 각
    > 레이어로부터의 투영은 네트워크에서 feature의 계층적인 특성을
    > 보여준다.

2.  Feature Evolution during Training : 다음으로 픽셀 공간에 투영된
    > feature map 내에서 가장 강력한 activation의 학습 중의 진행을
    > 시각화한다. 갑작스러운 모양의 변화는 가장 강력한 activation이
    > 발생하는 이미지의 변화에서 비롯된다. 모델의 하위 레이어는 적은
    > 에폭 내에 수렴하는 것을 확인할 수 있다. 그러나 상위 레이어의 경우
    > 상당한 기간 이후 (약 40 - 50 에폭) 개발되므로 모델이 완전히
    > 수렴할때까지 학습시킬 필요가 있음을 보여준다.

```{=html}
<!-- -->
```
A.  Arcitecture Selection

> 해당 모델의 시각화 과정에서 레이어 1, 2의 시각화는 다양한 문제를
> 유발할 수 있다. 첫번째 레이어 필터는 매우 높은 주파수와 낮은 주파수의
> 정보가 혼합되어 있으며 중간 주파수에 대한 적용 범위는 거의 존재하지
> 않는다. 또한 두번째 레이어 필터 시각화는 첫번째 레이어 convolution에
> 사용된 stride가 4로 설정되어 aliasing artifact를 보인다. 이때,
> aliasing artifact는 범위를 벗어난 구역에 이미지가 있을 경우 해당
> 이미지가 왜곡되는 현상을 의미한다. 이를 해결하기 위해 (1) 첫번째
> 레이어의 필터 크기를 11\*11에서 7\*7로 줄였고 (2) stride값을 4가 아닌
> 2로 수정하였다. 이러한 새로운 아키텍처는 첫번째, 두번째 레이어에서 더
> 많은 정보를 보유하게 된다. 더불어 중요한 것은 분류 성능 또한 개선된
> 점이다.

B. Occlusion Sensitivity

> 이미지 분류 학습에서 모델이 이미지에서 개체의 위치를 실제로
> 식별해내는지 혹은 주변의 context를 사용하는지 의문을 갖기 마련이다.
> 이에 대해 회색 사각형으로 입력 이미지의 일부분을 체계적으로 가리고
> 분류기의 출력을 모니터링하여 이 질문에 답하고자 시도한다. 이 예시는
> 객체가 가려질 때 올바른 클래스에 대한 확률이 현저히 떨어지기 때문에
> 모델이 이미지 내에서 객체를 지역화하고 있음을 명확히 보여준다. 또한
> occulder가 시각화에 나타나는 이미지 영역을 덮으면 feature map에서 해당
> 위치의 activation이 크게 감소하는 것을 보인다. 이는 시각화가 해당
> feauture map을 자극하는 이미지 구조와 실제로 일치함을 보여준다.

5.  Experiments

> ImageNet-1K 데이터셋을 활용하여 140 epoch을 학습시킨 모델을 사용하여
> 다양한 시각화 실험을 진행하고자 한다. 이때 실험에 사용한 주요 하이퍼
> 파라미터는 아래와 같다.

-   batch size : 280

-   learning rate : 0.05

-   num classes : 1000

학습 이후 검증 데이터셋에 대하여 최종 top 1 정확도는 약 40%, top 5
정확도는 17%를 보였다.

> 다양한 시각화 실험에 사용한 데이터셋의 경우 고양이와 강아지를 분류하는
> 작업에서 주로 사용되는 kaggle dogs vs cats 데이터 중 일부를 활용하고자
> 한다. 이를 통해 각 클래스 특징 별로 나누어 레이어 별 시각화를 진행하고
> 동일 이미지의 일부를 가려 이미지 분류 학습에서 모델이 이미지의 개체
> 위치를 실제로 식별해내는지 혹은 주변의 context를 사용하는 것인지
> 알아보고자 한다. 먼저 강아지 사진에 대한 원본 이미지는 제공된
> 데이터셋의 test셋 중 무작위로 추출하여 아래의 다섯개의 이미지를
> 활용하였다.![](media/image5.png){width="4.696875546806649in"
> height="1.3189851268591426in"}
>
> 각 이미지에 대한 Deconvolution layer 별 feature map 시각화는 아래의
> 그림과 같이 나타난다.![](media/image6.png){width="5.541666666666667in"
> height="5.466905074365704in"}
>
> 원본 이미지를 중앙의 224 \* 224 크기로 잘라내어 사용한 것을 확인할 수
> 있으며 각 레이어에서 추출한 특징들이 모두 상이한 것을 확인할 수 있다.
> 먼저 Deconvolution layer의 단계가 진행될수록 점차 구체적인 이미지의
> 모서리에 가까워지는 특성을 시각화 함을 알 수 있다. 이는 Deconvolution
> layer를 거치며 점차 원본 이미지로 복원되는 모습을 의미한다. 또한 각
> 레이어에서 보이고자 하는 특징이 상이함을 확인할 수 있다. Deconvolution
> layer 1, 2, 3의 경우 이미지에 등장하는 개체의 구체적인 내용이 아니라
> 그 전반적인 자세나 위치 등에 대해 시각화하는 경향이 있다. 반면
> Deconvolution layer 4, 5의 경우, 즉 Convolution layer의 입력층에
> 가까운 Deconvolution layer의 경우, 이미지에 등장하는 개체의 테두리
> 부분, 구체적인 생김새를 시각화하는 모습을 파악할 수 있다.
>
> 동일한 실험을 고양이 사진에 대해 진행한 결과는 아래와 같다. 순서대로
> 원본 이미지와 Deconvolution layer의 시각화
> 결과이다.![](media/image4.png){width="4.697916666666667in"
> height="1.0694444444444444in"}
>
> ![](media/image11.png){width="5.791666666666667in"
> height="5.720457130358705in"}
>
> 위에서 언급한 바와 같이, Deconvolution layer가 Convolution layer의
> 출력층에 가까워질수록 이미지에서 전반적으로 나타나는 특징들에 대해
> 시각화한 것을 볼 수 있고 Deconvolution layer가 Convolution layer의
> 입력층에 가까워질수록 이미지에 등장하는 개체의 구체적인 특징에 대해
> 시각화한 것을 볼 수 있다.
>
> 이후 Occlusion Sensitivity에 대해 실험하기 위해 이미지 가공 과정을
> 거쳤다. 각 이미지에 나타난 동물의 얼굴 위치를 회색 사각형으로 가린 뒤
> 각 레이어에서 나타나는 feature map 시각화 과정에 어떠한 차이가 있는지
> 살펴보고자 한다. 아래 사진들은 차례대로 얼굴을 마스킹한 강아지 사진,
> 마스킹한 강아지 사진에 대한 Deconvolution layer 별 feature map 시각화
> 결과, 얼굴을 마스킹한 고양이 원본 사진, 마스킹한 고양이 사진에 대한
> Deconvolution layer별 feature map 시각화
> 결과이다.![](media/image3.png){width="5.791666666666667in"
> height="1.6303138670166228in"}
>
> ![](media/image7.png){width="5.791666666666667in"
> height="5.722007874015748in"}
>
> ![](media/image2.png){width="5.791666666666667in"
> height="1.311321084864392in"}

![](media/image8.png){width="5.791666666666667in"
height="5.725096237970254in"}

> 우선, 마스킹 이전과 동일하게 각 레이어 별로 추출하는 feature의 차이가
> 있는 것을 확인할 수 있다.
>
> Deconvolution layer가 깊어질수록 Convolution layer의 입력층에
> 가까워지기 때문에 보다 구체적인 feature를 잡아내고 얕아질수록
> Convolution layer의 출력층에 가까워지기 때문에 보다 광범위한 이미지의
> feature를 잡아내는 것을 볼 수 있다. 그러나 마스킹 이전과는 다르게 그
> feature의 분포가 상이한 것을 확인할 수 있다. 이를 보다 구체적으로
> 확인하기 위해 동일 이미지의 마스킹 전후에 대한 Deconvolution layer별
> 시각화 결과의 차이를 비교해보고자 한다.

![](media/image1.png){width="6.346853674540682in"
height="2.474469597550306in"}

> 위의 사진은 실험에 사용한 다섯장의 강아지 이미지 중 3번째 이미지에
> 해당하는 사진과 Deconvolution layer별 시각화 결과이다. 이를 통해
> 논문의 내용을 보다 구체적으로 이해할 수 있다. 먼저 입력 이미지의
> 일부분을 회색 사각형으로 가리고 feature의 시각화를 진행한 결과
> 활성화되는 부분이 동일 입력 이미지일지라도 마스킹 여부에 따라 달라지는
> 것을 확인할 수 있다. 이를 통해 모델이 이미지 내에서 개체의 위치를
> 정확히 알고 학습을 진행하고 있음을 알 수 있다. 또한 마스킹을 한 지역이
> 다른 지역에 비해 활성화 정도가 상당히 떨어진 것을 볼 수 있다. 이를
> 통해 해당 시각화 결과가 feature map을 자극하는 이미지의 구조와 실제로
> 일치하는 것을 볼 수 있다.
>
> 더불어 마스킹 여부가 해당 이미지 분류의 올바른 클래스에 대한 확률에도
> 영향을 주고 있음을 확인할 수 있다. 위에서 사용한 세번째 강아지
> 이미지에 대해 마스킹 여부에 따른 분류 결과는 아래와 같다.
>
> ![](media/image10.png){width="6.395833333333333in"
> height="4.051043307086614in"}
>
> 원본 이미지는 American Stafford terrier에 해당하는 견종이다. 마스킹이
> 되지 않은 이미지의 경우 그 견종을 약 0.56%의 확률로 가장 높은 클래스
> 확률을 나타냈다. 반면, 마스킹이 된 이미지의 경우 상위 5개 클래스에
> 정답 클래스가 나타나지 않았을 뿐만 아니라 확률 자체가 약 0.16% 이하
> 수준으로 낮게 나타난다. 마스킹의 위치가 강아지의 얼굴에 위치하여 그
> 견종을 구분하는 데에 있어서 정확도의 차이가 크게 나타나는 것을 알 수
> 있다. 보다 다양한 실험을 위해 마스킹의 위치를 변경하여 분류 결과를
> 확인하였다. 그 결과는 아래의 사진과
> 같다.![](media/image9.png){width="4.5416054243219595in"
> height="4.0635422134733155in"}
>
> 마스킹의 위치를 얼굴이 아닌 하반신 쪽으로 변경하여 기존의 결과와
> 비교해보고자 한다. 그 결과 가장 높은 확률을 보이는 클래스의 경우
> 올바른 클래스인 American Stafford terrier로 나타났다. 그러나 그 확률이
> 마스킹하지 않은 이미지와 비교하여 낮게 나타나는 것을 볼 수 있다. 이는
> 마스킹으로 인해 해당 이미지를 분류해낼 때 사용하는 feature 중 일부가
> 지워져 모델이 분류에 있어 해당 feature를 고려하지 못하는 것을 확인할
> 수 있다.

6.  Discussion

> 본 논문은 이미지 분류를 위해 훈련된 대규모 Convolution Neural Network
> 모델을 여러가지 방법으로 탐색했다. 먼저, 모델 내에서 활동을 시각화할
> 수 있는 새로운 방법을 제시하였다. 이는 무작위적이고 해석이 불가능한
> 패턴과는 거리가 먼 특징을 보인다. 오히려, 레이어를 올라갈수록 구성성,
> 불변성 증가, 클래스 차별성과 같은 직관적으로 바람직한 많은 특성들을
> 보여준다. 또한 이러한 시각화를 사용하여 모델의 문제점을 식별한 뒤 이를
> 개선하여 더 나은 결과를 얻을 수 있는 방법을 제시한다. 이후 일련의
> occlusion 실험을 통해 분류를 위해 훈련된 모델이 이미지의 지역적인
> 구조에 매우 민감하며 넓은 이미지의 context만을 사용하는 것이 아님을
> 입증하였다. 또한, ablation 연구를 통해 개별 구간이 아닌 네트워크에
> 대한 최소 깊이가 모델의 성능에 필수적인 조건임을 확인하였다.
> 마지막으로 ImageNet으로 훈련된 모델이 다른 데이터 세트에 어떻게 잘
> 일반화될 수 있는지 보여주었다. Caltech-101과 Caltech-256의 경우 기존
> SOTA 모델을 능가할 정도로 충분히 유사한 성능을 보인다.
